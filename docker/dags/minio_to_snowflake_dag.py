import os
import boto3
import snowflake.connector
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# -------- MinIO Config --------
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY")
BUCKET = os.getenv("MINIO_BUCKET")
LOCAL_DIR = os.getenv("MINIO_LOCAL_DIR", "/tmp/minio_downloads")

# -------- Snowflake Config --------
SNOWFLAKE_USER = os.getenv("SNOWFLAKE_USER")
SNOWFLAKE_PASSWORD = os.getenv("SNOWFLAKE_PASSWORD")
SNOWFLAKE_ACCOUNT = os.getenv("SNOWFLAKE_ACCOUNT")
SNOWFLAKE_WAREHOUSE = os.getenv("SNOWFLAKE_WAREHOUSE")
SNOWFLAKE_DB = os.getenv("SNOWFLAKE_DB")
SNOWFLAKE_SCHEMA = os.getenv("SNOWFLAKE_SCHEMA")

TABLES = ["members", "policies", "claims"]

# -------- Python Callables --------
def download_from_minio():
    os.makedirs(LOCAL_DIR, exist_ok=True)
    s3 = boto3.client(
        "s3",
        endpoint_url=MINIO_ENDPOINT,
        aws_access_key_id=MINIO_ACCESS_KEY,
        aws_secret_access_key=MINIO_SECRET_KEY
    )
    local_files = {}
    for table in TABLES:
        prefix = f"{table}/"
        resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix)
        objects = resp.get("Contents", [])
        local_files[table] = []
        for obj in objects:
            key = obj["Key"]
            local_file = os.path.join(LOCAL_DIR, os.path.basename(key))
            s3.download_file(BUCKET, key, local_file)
            print(f"Downloaded {key} -> {local_file}")
            local_files[table].append(local_file)
    return local_files

def load_to_snowflake(**kwargs):
    local_files = kwargs["ti"].xcom_pull(task_ids="download_minio")
    if not local_files:
        print("No files found in MinIO.")
        return

    conn = snowflake.connector.connect(
        user=SNOWFLAKE_USER,
        password=SNOWFLAKE_PASSWORD,
        account=SNOWFLAKE_ACCOUNT,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DB,
        schema=SNOWFLAKE_SCHEMA,
    )
    cur = conn.cursor()

    # Drop and recreate tables with correct schema matching PostgreSQL source
    for table in TABLES:
        try:
            cur.execute(f"DROP TABLE IF EXISTS {table}")
            print(f"Dropped {table} table")
        except Exception as e:
            print(f"Error dropping {table}: {e}")

    # Create tables with schema matching PostgreSQL source columns
    table_schemas = {
        "members": """
            CREATE TABLE IF NOT EXISTS members (
                id INT,
                first_name VARCHAR(100),
                last_name VARCHAR(100),
                email VARCHAR(255),
                date_of_birth DATE,
                created_at TIMESTAMP_NTZ
            )
        """,
        "policies": """
            CREATE TABLE IF NOT EXISTS policies (
                id INT,
                member_id INT,
                policy_type VARCHAR(50),
                coverage_amount DECIMAL(18, 2),
                premium_amount DECIMAL(10, 2),
                policy_status VARCHAR(20),
                start_date DATE,
                end_date DATE,
                created_at TIMESTAMP_NTZ
            )
        """,
        "claims": """
            CREATE TABLE IF NOT EXISTS claims (
                id BIGINT,
                policy_id INT,
                claim_type VARCHAR(50),
                claim_amount DECIMAL(18, 2),
                approved_amount DECIMAL(18, 2),
                claim_status VARCHAR(20),
                service_date DATE,
                created_at TIMESTAMP_NTZ
            )
        """
    }

    for table, schema in table_schemas.items():
        print(f"Creating table {table} if not exists...")
        cur.execute(schema)

    for table, files in local_files.items():
        if not files:
            print(f"No files for {table}, skipping.")
            continue

        for f in files:
            cur.execute(f"PUT file://{f} @%{table}")
            print(f"Uploaded {f} -> @{table} stage")

        copy_sql = f"""
        COPY INTO {table}
        FROM @%{table}
        FILE_FORMAT=(TYPE=PARQUET)
        MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE
        ON_ERROR='SKIP_FILE'
        PURGE=TRUE
        """
        copy_result = cur.execute(copy_sql)
        rows_loaded = copy_result.rowcount
        print(f"Data loaded into {table}: {rows_loaded} rows")
        
        # Get copy command status
        status_result = cur.execute("SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))").fetchall()
        print(f"Copy status: {status_result}")
        
        # Verify data was actually loaded
        verify_sql = f"SELECT COUNT(*) as row_count FROM {table}"
        verify_result = cur.execute(verify_sql).fetchall()
        print(f"Verification: {table} now has {verify_result[0][0]} total rows")

    cur.close()
    conn.close()

# -------- Airflow DAG --------
default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

with DAG(
    dag_id="minio_to_snowflake_healthcare",
    default_args=default_args,
    description="Load MinIO parquet into Snowflake RAW tables",
    schedule_interval=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
) as dag:

    task1 = PythonOperator(
        task_id="download_minio",
        python_callable=download_from_minio,
    )

    task2 = PythonOperator(
        task_id="load_snowflake",
        python_callable=load_to_snowflake,
        provide_context=True,
    )

    task1 >> task2